{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ac37de-8ee0-4b0e-baff-9177f93e3798",
   "metadata": {},
   "source": [
    "# それぞれのLLMに最適化されたチャット プロンプトをテンプレートから効率的に作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d8a0d-c46b-4ea7-8c1d-c57f42613931",
   "metadata": {},
   "source": [
    "❗ **Google Colabで実行する場合、ランタイムのタイプはGPU（T4, V100, A100など）を指定すること**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125a088-2c6c-46b3-8cef-d766e49bbf34",
   "metadata": {},
   "source": [
    "**予め、Hugging Faceのウェブサイトにおいて、Meta Llama2およびMeta Llama3.1のライセンス許諾に同意し、モデルファイルのダウンロード許可を得る必要があります。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f8b1c-cf49-484c-a7ce-42a4ae2b5331",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497d6fa-4912-4066-8b07-9919a15cf005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hubにログイン\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb870e4-afdc-4e20-8571-de5097264447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m pip install --no-cache-dir --upgrade \\\n",
    "transformers \\\n",
    "langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa81f63-a48e-45f4-9925-e03618251c3d",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers Templates for Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b402b-565f-4dee-b6b0-3f096c162a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04558a-5845-42af-9d2a-ce4e61138fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"あなたは日本語ネイティブで親切なAIアシスタントです。\"},\n",
    "    {\"role\": \"user\", \"content\": \"こんにちは。ご機嫌いかがですか？\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"とても元気です。あなたのお役にたてることがあれば何なりとお尋ねください。\"},\n",
    "    {\"role\": \"user\", \"content\": \"大阪の知人へ贈る、東京の土産を提案してください。\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563def8f-eb82-4174-8fae-e0374643ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "tokenizer.use_default_system_prompt = False\n",
    "prompt_string = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "print(prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201f2e6-595d-4890-821d-e2158f38f1ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba186b4d-d8e1-47c3-93da-98b1817e2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.use_default_system_prompt = False\n",
    "prompt_string = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "print(prompt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50390b0f-dabc-4eef-a3f7-38490557965e",
   "metadata": {},
   "source": [
    "## LangChain Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ffadd-ae00-4193-a8df-2563a0910994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76c451-f738-4dbd-8abf-e907303a9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"あなたは日本語ネイティブで親切なAIアシスタントです。\"),\n",
    "    (\"user\", \"こんにちは。ご機嫌いかがですか？\"),\n",
    "    (\"assistant\", \"とても元気です。あなたのお役にたてることがあれば何なりとお尋ねください。\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6ca86-fdb3-4f77-9174-b1aec142ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value = prompt_template.invoke({\"question\": \"大阪の知人へ贈る、東京の土産を提案してください。\"})\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65946569-3057-4b11-9868-6ae30b944bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_list = prompt_value.to_messages()\n",
    "print(msg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8956a3-af78-4888-8eb4-0e98491887a6",
   "metadata": {},
   "source": [
    "## Templates for Chat ModelsをLangChainで使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86168dd5-9392-4ac8-9aee-5041d32b564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"あなたは日本語ネイティブで親切なAIアシスタントです。\"},\n",
    "    {\"role\": \"user\", \"content\": \"こんにちは。ご機嫌いかがですか？\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"とても元気です。あなたのお役にたてることがあれば何なりとお尋ねください。\"},\n",
    "    {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59143052-45d2-4291-8a70-af29e1a7ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "tokenizer.use_default_system_prompt = False\n",
    "prompt_string = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "print(prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6378ad-d8eb-4322-8906-7937149b3f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbcccce-4efc-4db7-adcd-458dc3904725",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(prompt_string)\n",
    "prompt_value = prompt_template.invoke({\"question\": \"大阪の知人へ贈る、東京の土産を提案してください。\"})\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd4ff5-787b-47f7-9719-08c5bbe714ec",
   "metadata": {},
   "source": [
    "## LLMサーバーでテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442db9d-96f5-47de-9987-ea9f4b01ce03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m pip install --no-cache-dir --upgrade \\\n",
    "langchain-openai \\\n",
    "\"llama-cpp-python[server]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80a3d4-260c-498f-93a8-1036c1fd7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download \\\n",
    "--local-dir models \\\n",
    "mmnga/Llama-3.1-8B-Instruct-gguf \\\n",
    "Llama-3.1-8B-Instruct-Q4_K_S.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5bd2c-5a2b-40dc-986d-b3d61e34af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# vLLMサーバーの起動コマンド\n",
    "command = [\n",
    "    \"python\", \"-m\", \"llama_cpp.server\",\n",
    "    \"--model\", \"models/Llama-3.1-8B-Instruct-Q4_K_S.gguf\",\n",
    "    \"--n_ctx\", \"2048\"\n",
    "]\n",
    "\n",
    "# バックグラウンドで実行\n",
    "process = subprocess.Popen(command)\n",
    "\n",
    "# サーバーが起動したことを通知\n",
    "print(\"vLLM server is running in the background.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3e882-2963-4a76-80a8-13d0710a9bb2",
   "metadata": {},
   "source": [
    "**以下のような表示が確認できたら次のセルを実行**  \n",
    "*INFO:     Started server process  \n",
    "INFO:     Waiting for application startup.  \n",
    "INFO:     Application startup complete.  \n",
    "INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b77788-fcef-4c91-a7d3-7f9313343ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl --request POST \\\n",
    "    --url http://localhost:8000/v1/chat/completions \\\n",
    "    --header \"Content-Type: application/json\" \\\n",
    "    --data '{\"prompt\": \"Hello\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1028a-3afd-4407-a8f8-102fb646262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf3b91-8794-46e6-89c7-496f02bbd753",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    name=\"Llama-3.1-8B-Instruct-Q4_K_S.gguf\",\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"DUMMY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8004bf5-2e5f-4bb4-88e8-b3c128f0a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain = prompt_template | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54b33b-809e-408d-b4bf-2ea029e85f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in chain.stream({\"question\": \"大阪の知人へ贈る、東京の土産を提案してください。\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464b27d-825d-4890-baba-0dcc570a9833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# サーバーを停止する\n",
    "process.terminate()\n",
    "print(\"vLLM server has been stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada8194-1f63-4472-9f6d-e170a3d236f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
