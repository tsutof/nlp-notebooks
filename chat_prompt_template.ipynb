{"cells":[{"cell_type":"markdown","id":"32ac37de-8ee0-4b0e-baff-9177f93e3798","metadata":{"id":"32ac37de-8ee0-4b0e-baff-9177f93e3798"},"source":["# LLM毎に最適化されたチャット プロンプトをテンプレートから効率的に作成する"]},{"cell_type":"markdown","id":"911d8a0d-c46b-4ea7-8c1d-c57f42613931","metadata":{"id":"911d8a0d-c46b-4ea7-8c1d-c57f42613931"},"source":["❗ **Google Colabで実行する場合、ランタイムのタイプはGPU（T4, A100, L4など）を指定すること**"]},{"cell_type":"markdown","id":"9125a088-2c6c-46b3-8cef-d766e49bbf34","metadata":{"id":"9125a088-2c6c-46b3-8cef-d766e49bbf34"},"source":["**予め、[Hugging Faceのウェブサイト](https://huggingface.co/meta-llama)において、Meta Llama2およびMeta Llama3.1のライセンス許諾に同意し、モデルファイルのダウンロード許可を得る必要があります。**"]},{"cell_type":"markdown","id":"362f8b1c-cf49-484c-a7ce-42a4ae2b5331","metadata":{"id":"362f8b1c-cf49-484c-a7ce-42a4ae2b5331"},"source":["## 準備"]},{"cell_type":"markdown","source":["必要なPythonパッケージをインストールします。"],"metadata":{"id":"1hxtk4p6bK40"},"id":"1hxtk4p6bK40"},{"cell_type":"code","execution_count":null,"id":"ecb870e4-afdc-4e20-8571-de5097264447","metadata":{"scrolled":true,"id":"ecb870e4-afdc-4e20-8571-de5097264447"},"outputs":[],"source":["!python -m pip install --no-cache-dir --upgrade \\\n","huggingface_hub \\\n","transformers \\\n","langchain \\\n","langchain-community"]},{"cell_type":"markdown","source":["Hugging Face Hubにログインします。"],"metadata":{"id":"ktBe70IJa4iK"},"id":"ktBe70IJa4iK"},{"cell_type":"code","execution_count":null,"id":"3497d6fa-4912-4066-8b07-9919a15cf005","metadata":{"id":"3497d6fa-4912-4066-8b07-9919a15cf005"},"outputs":[],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"markdown","id":"6fa81f63-a48e-45f4-9925-e03618251c3d","metadata":{"id":"6fa81f63-a48e-45f4-9925-e03618251c3d"},"source":["## Hugging Face Transformers Templates for Chat Models"]},{"cell_type":"code","execution_count":null,"id":"798b402b-565f-4dee-b6b0-3f096c162a9c","metadata":{"id":"798b402b-565f-4dee-b6b0-3f096c162a9c"},"outputs":[],"source":["from transformers import AutoTokenizer"]},{"cell_type":"markdown","source":["チャットのロールと内容を定義します。"],"metadata":{"id":"5mFIGR-obUuB"},"id":"5mFIGR-obUuB"},{"cell_type":"code","execution_count":null,"id":"3c04558a-5845-42af-9d2a-ce4e61138fd3","metadata":{"id":"3c04558a-5845-42af-9d2a-ce4e61138fd3"},"outputs":[],"source":["chat = [\n","    {\"role\": \"system\", \"content\": \"あなたは日本語ネイティブで親切なAIアシスタントです。\"},\n","    {\"role\": \"user\", \"content\": \"こんにちは。ご機嫌いかがですか？\"},\n","    {\"role\": \"assistant\", \"content\": \"とても元気です。あなたのお役にたてることがあれば何なりとお尋ねください。\"},\n","    {\"role\": \"user\", \"content\": \"大阪の知人へ贈る、東京の土産を提案してください。\"},\n","]"]},{"cell_type":"markdown","source":["Llama 3.1 8B Instructモデルのトーカナイザーでチャットプロンプトを生成します。"],"metadata":{"id":"D3s8_oytbaUH"},"id":"D3s8_oytbaUH"},{"cell_type":"code","execution_count":null,"id":"563def8f-eb82-4174-8fae-e0374643ff88","metadata":{"id":"563def8f-eb82-4174-8fae-e0374643ff88"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n","tokenizer.use_default_system_prompt = False\n","prompt_string = tokenizer.apply_chat_template(chat, tokenize=False)\n","print(prompt_string)"]},{"cell_type":"markdown","source":["Jinjaテンプレートを表示してみます。"],"metadata":{"id":"bpLAt0AsbfO8"},"id":"bpLAt0AsbfO8"},{"cell_type":"code","execution_count":null,"id":"7201f2e6-595d-4890-821d-e2158f38f1ff","metadata":{"scrolled":true,"id":"7201f2e6-595d-4890-821d-e2158f38f1ff"},"outputs":[],"source":["print(tokenizer.chat_template)"]},{"cell_type":"markdown","source":["Llama 2 7B Chatモデルのトーカナイザーでチャットプロンプトを生成します。"],"metadata":{"id":"fuRRUKQlbpXU"},"id":"fuRRUKQlbpXU"},{"cell_type":"code","execution_count":null,"id":"ba186b4d-d8e1-47c3-93da-98b1817e2396","metadata":{"id":"ba186b4d-d8e1-47c3-93da-98b1817e2396"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n","tokenizer.use_default_system_prompt = False\n","prompt_string = tokenizer.apply_chat_template(chat, tokenize=False)\n","print(prompt_string)"]},{"cell_type":"markdown","id":"50390b0f-dabc-4eef-a3f7-38490557965e","metadata":{"id":"50390b0f-dabc-4eef-a3f7-38490557965e"},"source":["## LangChain Prompt Templates"]},{"cell_type":"code","execution_count":null,"id":"2a4ffadd-ae00-4193-a8df-2563a0910994","metadata":{"id":"2a4ffadd-ae00-4193-a8df-2563a0910994"},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate"]},{"cell_type":"markdown","source":["チャットのロールと内容をテンプレートに定義します。"],"metadata":{"id":"wfX6uBn4dIqH"},"id":"wfX6uBn4dIqH"},{"cell_type":"code","execution_count":null,"id":"6a76c451-f738-4dbd-8abf-e907303a9421","metadata":{"id":"6a76c451-f738-4dbd-8abf-e907303a9421"},"outputs":[],"source":["prompt_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"あなたは日本語ネイティブで親切なAIアシスタントです。\"),\n","    (\"user\", \"こんにちは。ご機嫌いかがですか？\"),\n","    (\"assistant\", \"とても元気です。あなたのお役にたてることがあれば何なりとお尋ねください。\"),\n","    (\"user\", \"{question}\")\n","])"]},{"cell_type":"markdown","source":["テンプレートからチャットプロンプトを生成します。"],"metadata":{"id":"vkSo2B7wdZw2"},"id":"vkSo2B7wdZw2"},{"cell_type":"code","execution_count":null,"id":"efd6ca86-fdb3-4f77-9174-b1aec142ef68","metadata":{"id":"efd6ca86-fdb3-4f77-9174-b1aec142ef68"},"outputs":[],"source":["prompt_value = prompt_template.invoke({\"question\": \"大阪の知人へ贈る、東京の土産を提案してください。\"})\n","print(prompt_value.to_string())"]},{"cell_type":"markdown","source":["チャットプロンプトの内容をリストで取り出すこともできます。"],"metadata":{"id":"wgOLSe1Cd5zd"},"id":"wgOLSe1Cd5zd"},{"cell_type":"code","execution_count":null,"id":"65946569-3057-4b11-9868-6ae30b944bb2","metadata":{"id":"65946569-3057-4b11-9868-6ae30b944bb2"},"outputs":[],"source":["msg_list = prompt_value.to_messages()\n","print(msg_list)"]},{"cell_type":"markdown","id":"9c8956a3-af78-4888-8eb4-0e98491887a6","metadata":{"id":"9c8956a3-af78-4888-8eb4-0e98491887a6"},"source":["## Templates for Chat ModelsをLangChainで使う"]},{"cell_type":"markdown","source":["チャットのロールと内容を定義します。"],"metadata":{"id":"8ikQANEeeLoO"},"id":"8ikQANEeeLoO"},{"cell_type":"code","execution_count":null,"id":"86168dd5-9392-4ac8-9aee-5041d32b564a","metadata":{"id":"86168dd5-9392-4ac8-9aee-5041d32b564a"},"outputs":[],"source":["chat = [\n","    {\"role\": \"system\", \"content\": \"あなたは日本語ネイティブで親切なAIアシスタントです。\"},\n","    {\"role\": \"user\", \"content\": \"こんにちは。ご機嫌いかがですか？\"},\n","    {\"role\": \"assistant\", \"content\": \"とても元気です。あなたのお役にたてることがあれば何なりとお尋ねください。\"},\n","    {\"role\": \"user\", \"content\": \"{question}\"},\n","]"]},{"cell_type":"markdown","source":["Llama 3.1 8B Instructモデルのトーカナイザーでチャットプロンプトを生成します。"],"metadata":{"id":"wV_-4nLhejd0"},"id":"wV_-4nLhejd0"},{"cell_type":"code","execution_count":null,"id":"59143052-45d2-4291-8a70-af29e1a7ba8c","metadata":{"id":"59143052-45d2-4291-8a70-af29e1a7ba8c"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n","tokenizer.use_default_system_prompt = False\n","prompt_string = tokenizer.apply_chat_template(chat, tokenize=False)\n","print(prompt_string)"]},{"cell_type":"code","execution_count":null,"id":"cc6378ad-d8eb-4322-8906-7937149b3f83","metadata":{"id":"cc6378ad-d8eb-4322-8906-7937149b3f83"},"outputs":[],"source":["from langchain_core.prompts import PromptTemplate"]},{"cell_type":"markdown","source":["生成したプロンプトをLangChain形式のプロンプトテンプレートに変換します。"],"metadata":{"id":"rPlLIiFce1-7"},"id":"rPlLIiFce1-7"},{"cell_type":"code","execution_count":null,"id":"dcbcccce-4efc-4db7-adcd-458dc3904725","metadata":{"id":"dcbcccce-4efc-4db7-adcd-458dc3904725"},"outputs":[],"source":["prompt_template = PromptTemplate.from_template(prompt_string)\n","prompt_value = prompt_template.invoke({\"question\": \"大阪の知人へ贈る、東京の土産を提案してください。\"})\n","print(prompt_value.to_string())"]},{"cell_type":"markdown","id":"4cbd4ff5-787b-47f7-9719-08c5bbe714ec","metadata":{"id":"4cbd4ff5-787b-47f7-9719-08c5bbe714ec"},"source":["## Llama 3.1 8B Instruct 4ビット量子化モデルでテスト"]},{"cell_type":"markdown","source":["### llama-cpp-pythonのインストール\n","※実行環境により方法が異なります。以下の３通りから**該当する方法のみ**を実行してください。  \n","詳細は[llama-cpp-python GitHubリポジトリ](https://github.com/abetlen/llama-cpp-python)を参照してください。"],"metadata":{"id":"4xxH8A-nLeeQ"},"id":"4xxH8A-nLeeQ"},{"cell_type":"markdown","source":["#### **NVIDIA CUDA GPUの場合**"],"metadata":{"id":"nhrPVU91OnbT"},"id":"nhrPVU91OnbT"},{"cell_type":"markdown","source":["まず、nvidia-smiコマンドでCUDAバージョンを確認します。"],"metadata":{"id":"T7qmGBDmQbI2"},"id":"T7qmGBDmQbI2"},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"rXin4nbCJBy5"},"id":"rXin4nbCJBy5","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["次に、ビルド済みWheelからllama-cpp-pythonをインストールします。WheelファイルのURL末尾はCUDAバージョンにより以下のとおりです。\n","- `cu121`: CUDA 12.1\n","- `cu122`: CUDA 12.2\n","- `cu123`: CUDA 12.3\n","- `cu124`: CUDA 12.4\n","\n","以下のセルで**該当するバージョンのみコメントを外し**有効にしてください。"],"metadata":{"id":"hsaNJ2NqQwdO"},"id":"hsaNJ2NqQwdO"},{"cell_type":"code","source":["# !python -m pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n","!python -m pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n","# !python -m pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123\n","# !python -m pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124"],"metadata":{"id":"wPaeWaY0JWeA"},"id":"wPaeWaY0JWeA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NVIDIA CUDA GPU環境の場合のllama-cpp-pythonインストールは以上です。  \n","「LangChain Expression Language (LCEL) チェインの構築」へ進んでください。"],"metadata":{"id":"4K3cj6Yfeu_a"},"id":"4K3cj6Yfeu_a"},{"cell_type":"markdown","source":["#### **Mac (Metal) の場合**"],"metadata":{"id":"mpL3R2MoMNVk"},"id":"mpL3R2MoMNVk"},{"cell_type":"code","source":["!CMAKE_ARGS=\"-DGGML_METAL=on\" python -m pip install --no-cache-dir --upgrade llama-cpp-python"],"metadata":{"id":"Tw446HtDMian"},"id":"Tw446HtDMian","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mac (Metal) 環境の場合のllama-cpp-pythonインストールは以上です。  \n","「LangChain Expression Language (LCEL) チェインの構築」へ進んでください。"],"metadata":{"id":"8wUBTGNNfPHN"},"id":"8wUBTGNNfPHN"},{"cell_type":"markdown","source":["#### **CPUのみの場合**"],"metadata":{"id":"VV1MmiRoUTy7"},"id":"VV1MmiRoUTy7"},{"cell_type":"code","source":["!python -m pip install --no-cache-dir --upgrade llama-cpp-python"],"metadata":{"id":"BuBHlCt_Ua5t"},"id":"BuBHlCt_Ua5t","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CPUのみ環境の場合のllama-cpp-pythonインストールは以上です。  \n","「LangChain Expression Language (LCEL) チェインの構築」へ進んでください。"],"metadata":{"id":"z8dcIwmcfZkJ"},"id":"z8dcIwmcfZkJ"},{"cell_type":"markdown","source":["### LangChain Expression Language (LCEL) チェインの構築"],"metadata":{"id":"zhrLRFZHOyft"},"id":"zhrLRFZHOyft"},{"cell_type":"code","execution_count":null,"id":"acc1028a-3afd-4407-a8f8-102fb646262f","metadata":{"id":"acc1028a-3afd-4407-a8f8-102fb646262f"},"outputs":[],"source":["from huggingface_hub import hf_hub_download\n","from langchain_community.llms import LlamaCpp\n","from langchain_core.output_parsers import StrOutputParser"]},{"cell_type":"markdown","source":["モデルをダウンロードします。"],"metadata":{"id":"CRXhSJ_KieJv"},"id":"CRXhSJ_KieJv"},{"cell_type":"code","execution_count":null,"id":"c4b2d1dc-3ab5-4d08-a407-9129a648a8e3","metadata":{"id":"c4b2d1dc-3ab5-4d08-a407-9129a648a8e3"},"outputs":[],"source":["model_path = hf_hub_download(\n","    repo_id=\"mmnga/Llama-3.1-8B-Instruct-gguf\",\n","    filename=\"Llama-3.1-8B-Instruct-Q4_K_S.gguf\"\n",")"]},{"cell_type":"markdown","source":["モデルをロードします。"],"metadata":{"id":"po61lVL0imNv"},"id":"po61lVL0imNv"},{"cell_type":"code","execution_count":null,"id":"b91ef942-ecf1-4523-9d6c-b61256f002b7","metadata":{"id":"b91ef942-ecf1-4523-9d6c-b61256f002b7"},"outputs":[],"source":["llm = LlamaCpp(\n","    model_path=model_path,\n","    n_gpu_layers=-1,\n","    n_ctx=2048,\n","    f16_kv=True,\n","    verbose=True\n",")"]},{"cell_type":"markdown","source":["LCELチェインを構築します。"],"metadata":{"id":"aDsDrsx4ir1F"},"id":"aDsDrsx4ir1F"},{"cell_type":"code","execution_count":null,"id":"f8004bf5-2e5f-4bb4-88e8-b3c128f0a317","metadata":{"id":"f8004bf5-2e5f-4bb4-88e8-b3c128f0a317"},"outputs":[],"source":["output_parser = StrOutputParser()\n","chain = prompt_template | llm | output_parser"]},{"cell_type":"markdown","source":["### 推論"],"metadata":{"id":"VXri8Et-Pw6i"},"id":"VXri8Et-Pw6i"},{"cell_type":"code","execution_count":null,"id":"0b54b33b-809e-408d-b4bf-2ea029e85f9d","metadata":{"id":"0b54b33b-809e-408d-b4bf-2ea029e85f9d"},"outputs":[],"source":["for s in chain.stream({\"question\": \"大阪の知人へ贈る、東京の土産を提案してください。\"}):\n","    print(s, end=\"\", flush=True)"]},{"cell_type":"code","source":["for s in chain.stream({\"question\": \"健康的な最強の朝食メニューを考えてください。\"}):\n","    print(s, end=\"\", flush=True)"],"metadata":{"id":"iKICWXAOWiBa"},"id":"iKICWXAOWiBa","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/tsutof/nlp-notebooks/blob/main/chat_prompt_template.ipynb","timestamp":1723699605332}],"gpuType":"T4","collapsed_sections":["nhrPVU91OnbT","mpL3R2MoMNVk"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}